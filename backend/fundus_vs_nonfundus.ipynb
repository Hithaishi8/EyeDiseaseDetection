{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b82278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fundus images loaded: 4152\n",
      "ðŸ’¡ Bright non-fundus images loaded: 37\n",
      "Files already downloaded and verified\n",
      "ðŸ“¦ CIFAR10 non-fundus images loaded: 50000\n",
      "ðŸ“Š Total dataset size: 8341\n",
      "ðŸŸ¦ Train: 6672 | ðŸŸ© Val: 1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:25<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Epoch 1 | Train Loss: 0.0287\n",
      "âœ… Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [20:26<00:00,  2.94s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Epoch 2 | Train Loss: 0.0009\n",
      "âœ… Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [05:25<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Epoch 3 | Train Loss: 0.0014\n",
      "âœ… Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [05:23<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Epoch 4 | Train Loss: 0.0002\n",
      "âœ… Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [05:18<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Epoch 5 | Train Loss: 0.0001\n",
      "âœ… Validation Accuracy: 100.00%\n",
      "\n",
      "ðŸŽ‰ Model saved â†’ C:\\Users\\hitha\\OneDrive\\Desktop\\eye app\\eye disease app\\backend\\models\\fundus_vs_nonfundus.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FUNDUS vs NON-FUNDUS CLASSIFIER TRAINER (UPDATED VERSION)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "FUNDUS_DIR = r\"C:\\Users\\hitha\\OneDrive\\Desktop\\eye app\\eye disease app\\dataset\"\n",
    "BRIGHT_DIR = r\"C:\\Users\\hitha\\OneDrive\\Desktop\\eye app\\eye disease app\\backend\\bright_nonfundus\"\n",
    "\n",
    "SAVE_PATH = r\"C:\\Users\\hitha\\OneDrive\\Desktop\\eye app\\eye disease app\\backend\\models\\fundus_vs_nonfundus.pt\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# DATA TRANSFORMS\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD FUNDUS IMAGES\n",
    "# -----------------------------\n",
    "fundus_dataset = datasets.ImageFolder(FUNDUS_DIR, transform=transform)\n",
    "fundus_dataset.samples = [(path, 1) for path, _ in fundus_dataset.samples]\n",
    "print(f\"âœ… Fundus images loaded: {len(fundus_dataset)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD BRIGHT NON-FUNDUS IMAGES\n",
    "# -----------------------------\n",
    "bright_dataset = datasets.ImageFolder(BRIGHT_DIR, transform=transform)\n",
    "\n",
    "# label = 0 for NON-FUNDUS\n",
    "bright_dataset.samples = [(path, 0) for path, _ in bright_dataset.samples]\n",
    "\n",
    "print(f\"ðŸ’¡ Bright non-fundus images loaded: {len(bright_dataset)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD CIFAR10 AS GENERAL NON-FUNDUS\n",
    "# -----------------------------\n",
    "cifar_data = datasets.CIFAR10(\n",
    "    root=\"./cifar_data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“¦ CIFAR10 non-fundus images loaded: {len(cifar_data)}\")\n",
    "\n",
    "# Make CIFAR subset same size as fundus dataset\n",
    "cifar_subset, _ = random_split(\n",
    "    cifar_data,\n",
    "    [len(fundus_dataset), len(cifar_data) - len(fundus_dataset)]\n",
    ")\n",
    "\n",
    "# Wrap CIFAR labels -> 0\n",
    "class CifarNonFundus(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.subset[idx]\n",
    "        return x, 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "cifar_wrapped = CifarNonFundus(cifar_subset)\n",
    "\n",
    "# -----------------------------\n",
    "# COMBINE DATASETS\n",
    "# -----------------------------\n",
    "combined_dataset = ConcatDataset([\n",
    "    fundus_dataset,\n",
    "    bright_dataset,\n",
    "    cifar_wrapped\n",
    "])\n",
    "\n",
    "print(f\"ðŸ“Š Total dataset size: {len(combined_dataset)}\")\n",
    "\n",
    "train_len = int(0.8 * len(combined_dataset))\n",
    "val_len = len(combined_dataset) - train_len\n",
    "\n",
    "train_set, val_set = random_split(combined_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"ðŸŸ¦ Train: {len(train_set)} | ðŸŸ© Val: {len(val_set)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL â€” MobileNet V3 Small\n",
    "# -----------------------------\n",
    "model = models.mobilenet_v3_small(\n",
    "    weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    ")\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 2)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"ðŸ§  Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"âœ… Validation Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE MODEL\n",
    "# -----------------------------\n",
    "os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Model saved â†’ {SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
